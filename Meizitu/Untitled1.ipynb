{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'attrs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-88ae209c4f8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;31m# 获取总页数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m \u001b[0mpages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_total_pages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;31m# 爬取IP代理\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-88ae209c4f8d>\u001b[0m in \u001b[0;36mget_total_pages\u001b[1;34m(first_url)\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[0mbsop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[0mlis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbsop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'wp_page_numbers'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ul'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'li'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m     \u001b[0mpages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m     \u001b[0mpages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'attrs'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "'''\n",
    "python 3.5.2\n",
    "'''\n",
    "\n",
    "# 导入模块\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def ip_test(ip, url_for_test='https://www.baidu.com', set_timeout=10):\n",
    "    '''\n",
    "    检测爬取到的ip地址可否使用，能使用返回True，否则返回False，默认去访问百度测试代理\n",
    "    :param ip:\n",
    "    :param url_for_test:\n",
    "    :param set_timeout:\n",
    "    :return:\n",
    "    '''\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            url_for_test,\n",
    "            headers=headers,\n",
    "            proxies={\n",
    "                'http': ip[0] + ':' + ip[1]},\n",
    "            timeout=set_timeout)\n",
    "        if r.status_code == 200:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except BaseException:\n",
    "        return False\n",
    "\n",
    "\n",
    "def scrawl_ip(url, num, url_for_test='https://www.baidu.com'):\n",
    "    '''\n",
    "    爬取代理ip地址，代理的url是西祠代理\n",
    "    :param url:\n",
    "    :param num:\n",
    "    :param url_for_test:\n",
    "    :return:\n",
    "    '''\n",
    "    ip_list = []\n",
    "    for num_page in range(3, num + 1):\n",
    "        url = url + str(num_page)\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.encoding = 'utf-8'\n",
    "        content = response.text\n",
    "\n",
    "        pattern = re.compile(\n",
    "            '<td class=\"country\">.*?alt=\"Cn\" />.*?</td>.*?<td>(.*?)</td>.*?<td>(.*?)</td>',\n",
    "            re.S)\n",
    "        items = re.findall(pattern, content)\n",
    "        for ip in items:\n",
    "            if ip_test(ip[1], url_for_test):  # 测试爬取到ip是否可用，测试通过则加入ip_list列表之中\n",
    "                print('测试通过，IP地址为' + str(ip[0]) + ':' + str(ip[1]))\n",
    "                ip_list.append(ip[0] + ':' + ip[1])\n",
    "        return ip_list\n",
    "\n",
    "    time.sleep(5)  # 等待5秒爬取下一页\n",
    "\n",
    "\n",
    "def get_random_ip():    # 随机获取一个IP\n",
    "    ind = random.randint(0, len(total_ip) - 1)\n",
    "    return total_ip[ind]\n",
    "\n",
    "\n",
    "def download_img(img_list, img_title):\n",
    "    '''\n",
    "    通过scrawl_url函数获得了单个图册里面所有图片的url列表和图册的名字，就可以下载图片了\n",
    "    此函数的作用下载单个图册里面的所有图片\n",
    "    接收参数img_list是单个图册里面所有图片的的url，\n",
    "    如['http://mm.howkuai.com/wp-content/uploads/2017a/02/07/01.jpg',\n",
    "    'http://mm.howkuai.com/wp-content/uploads/2017a/02/07/02.jpg',...]\n",
    "    img_title是单个图册的名字，如’香车美女，最完美的黄金搭档‘\n",
    "    :param img_list:\n",
    "    :param img_title:\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    img_title = format_name(img_title)  # 如果图册名字有特殊字符需要处理。不然在windows下保存不了文件夹\n",
    "    for img_urls in img_list:\n",
    "        img_url = img_urls.attrs['src']  # 单个图片的url地址\n",
    "        print(img_url)\n",
    "        title = img_urls.attrs['alt']  # 单个图片的名字\n",
    "        print(title)\n",
    "\n",
    "        try:\n",
    "            if not os.path.exists(os.path.join(file_path, img_title)):\n",
    "                os.makedirs(os.path.join(file_path, img_title))\n",
    "            os.chdir(file_path + '\\\\' + img_title)\n",
    "\n",
    "            # 图片保存到本地\n",
    "            exists = os.path.exists(img_title)\n",
    "            if not exists:\n",
    "                try:\n",
    "                    img_html = requests.get(\n",
    "                        img_url, headers=headers, stream=True, timeout=20, verify=True)\n",
    "                    with open(title + \".jpg\", 'wb') as f:\n",
    "                        f.write(img_html.content)\n",
    "                        f.close()\n",
    "                except BaseException:\n",
    "                    continue\n",
    "        except BaseException:\n",
    "            continue\n",
    "\n",
    "\n",
    "def scrawl_list(url_list, proxy_flag=False, try_time=0):\n",
    "    '''\n",
    "    此函数的作用是爬取每一页面所有图册的url，一个页面包含10个图册，所有调用一次函数则返回一个包含10个url的列表\n",
    "    格式如['http://www.meizitu.com/a/list_1_1.html',...]\n",
    "    :param url_list:\n",
    "    :param proxy_flag:\n",
    "    :param try_time:\n",
    "    :return:\n",
    "    '''\n",
    "    if not proxy_flag:  # 不使用代理\n",
    "        try:\n",
    "            html = requests.get(url_list, headers=headers, timeout=10)\n",
    "            html.encoding = 'gb2312'\n",
    "            text = html.text\n",
    "\n",
    "            bsop = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "            url_imgs = []\n",
    "            li_list = bsop.find('ul',\n",
    "                                {'class': 'wp-list clearfix'}).findAll('li',\n",
    "                                                                       {'class': 'wp-item'})\n",
    "            for i in li_list:\n",
    "                url_img = i.find('h3', {'class': 'tit'}\n",
    "                                 ).find('a').attrs['href']\n",
    "                url_imgs.append(url_img)\n",
    "            return url_imgs\n",
    "        except BaseException:\n",
    "            return scrawl_list(url_list, proxy_flag=True)  # 否则调用自己，使用3次IP代理\n",
    "    else:   # 使用代理时\n",
    "        if try_time < count_time:\n",
    "            try:\n",
    "                print('尝试第' + str(try_time + 1) + '次使用代理下载')\n",
    "                html = requests.get(\n",
    "                    url_list, headers=headers, proxies={\n",
    "                        'http': get_random_ip()}, timeout=10)\n",
    "                html.encoding = 'gb2312'\n",
    "                text = html.text\n",
    "\n",
    "                bsop = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "                url_imgs = []\n",
    "                # url_titles = []\n",
    "                li_list = bsop.find('ul',\n",
    "                                    {'class': 'wp-list clearfix'}).findAll('li',\n",
    "                                                                           {'class': 'wp-item'})\n",
    "                for i in li_list:\n",
    "                    url_img = i.find('h3',\n",
    "                                     {'class': 'tit'}).find('a').attrs['href']\n",
    "                    url_imgs.append(url_img)\n",
    "                print('状态码为' + str(html.status_code))\n",
    "                if html.status_code == 200:\n",
    "                    print('url_imgs通过IP代理处理成功！')\n",
    "                    return url_imgs  # 代理成功下载！\n",
    "                else:\n",
    "                    return scrawl_list(\n",
    "                        url_list, proxy_flag=True, try_time=(\n",
    "                            try_time + 1))\n",
    "            except BaseException:\n",
    "                print('url_imgs代理下载失败，尝试下次代理')\n",
    "                return scrawl_list(\n",
    "                    url_list, proxy_flag=True, try_time=(\n",
    "                        try_time + 1))  # 否则调用自己，使用3次IP代理\n",
    "        else:\n",
    "            print('url_imgs爬取失败，请检查网页')\n",
    "            return None\n",
    "\n",
    "\n",
    "def scrawl_url(url, proxy_flag=False, try_time=0):\n",
    "    '''\n",
    "    此函数的作用是爬取单个图册里面的所有图片的url，一个图册包含几张图片，每个图片有个真实的url地址，需要获取得到\n",
    "    此函数接收图册url作为参数，如'http://www.meizitu.com/a/5499.html',返回该图册里面所有图片的url列表和图册的名字\n",
    "    所有图片共用一个名字，可作为文件夹名字存储\n",
    "    :param url:\n",
    "    :param proxy_flag:\n",
    "    :param try_time:\n",
    "    :return:\n",
    "    '''\n",
    "    if not proxy_flag:  # 不使用代理\n",
    "        try:\n",
    "            html = requests.get(url, headers=headers, timeout=10)\n",
    "            html.encoding = 'gb2312'\n",
    "            text = html.text\n",
    "\n",
    "            bsop = BeautifulSoup(text, 'html.parser')\n",
    "            img_list = bsop.find('div', {'class': 'postContent'}).find(\n",
    "                'p').findAll('img')\n",
    "            img_title = bsop.find('div', {'class': 'metaRight'}).find(\n",
    "                'h2').find('a').text\n",
    "\n",
    "            return img_list, img_title\n",
    "\n",
    "        except BaseException:\n",
    "            return scrawl_url(url, proxy_flag=True)  # 否则调用自己，使用3次IP代理\n",
    "    else:   # 使用代理时\n",
    "        if try_time < count_time:\n",
    "            try:\n",
    "                print('尝试第' + str(try_time + 1) + '次使用代理下载')\n",
    "\n",
    "                html = requests.get(\n",
    "                    url, headers=headers, proxies={\n",
    "                        'http': get_random_ip()}, timeout=30)\n",
    "                html.encoding = 'gb2312'\n",
    "\n",
    "                text = html.text\n",
    "                bsop = BeautifulSoup(text, 'html.parser')\n",
    "                img_list = bsop.find('div', {'class': 'postContent'}).find(\n",
    "                    'p').findAll('img')\n",
    "                img_title = bsop.find('div', {'class': 'metaRight'}).find(\n",
    "                    'h2').find('a').text\n",
    "\n",
    "                print('状态码为' + str(html.status_code))\n",
    "                if html.status_code == 200:\n",
    "                    print('图片通过IP代理处理成功！')\n",
    "                    return img_list, img_title  # 代理成功下载！\n",
    "                else:\n",
    "                    return scrawl_url(\n",
    "                        url, proxy_flag=True, try_time=(\n",
    "                            try_time + 1))\n",
    "            except BaseException:\n",
    "                print('IP代理下载失败')\n",
    "                return scrawl_url(\n",
    "                    url, proxy_flag=True, try_time=(\n",
    "                        try_time + 1))  # 否则调用自己，使用3次IP代理\n",
    "        else:\n",
    "            print('图片url列表未能爬取，请检查网页')\n",
    "            return None\n",
    "\n",
    "\n",
    "def download_urls(pages):\n",
    "    '''\n",
    "     此函数的作用是爬取所有页面的url，最后返回的是包含所有页面url的二位列表，格式如下\n",
    "     url_imgss = [\n",
    "                  ['http://www.meizitu.com/a/list_1_1.html',...],\n",
    "                  ['http://www.meizitu.com/a/list_1_2.html',...],\n",
    "                  ...\n",
    "                 ]\n",
    "    '''\n",
    "    url_imgss = []\n",
    "    for i in range(1, pages + 1):\n",
    "        try:\n",
    "            url_list = prefix_url + str(i) + '.html'\n",
    "            url_imgs = scrawl_list(url_list)\n",
    "            if not url_imgs:\n",
    "                continue\n",
    "            url_imgss.append(url_imgs)\n",
    "            print(\"第\" + str(i) + \"页url爬取成功\")\n",
    "            time.sleep(5)  # 休息5秒篇爬取下一页\n",
    "        except BaseException:  # 如果其中某一页出错，则跳过该页，继续爬取下一页，从而不使程序中断\n",
    "            continue\n",
    "    return url_imgss\n",
    "\n",
    "\n",
    "def format_name(img_title):\n",
    "    '''\n",
    "    对名字进行处理，如果包含下属字符，则直接剔除该字符\n",
    "    :param img_title:\n",
    "    :return:\n",
    "    '''\n",
    "    for i in ['\\\\', '/', ':', '*', '?', '\"', '<', '>', '!', '|']:\n",
    "        while i in img_title:\n",
    "            img_title = img_title.strip().replace(i, '')\n",
    "    return img_title\n",
    "\n",
    "\n",
    "def get_total_pages(first_url):\n",
    "    '''\n",
    "    获取妹子图所有页面\n",
    "    :param first_url:\n",
    "    :return:\n",
    "    '''\n",
    "    html = requests.get(first_url, headers=headers, timeout=10)\n",
    "    html.encoding = 'gb2312'\n",
    "    text = html.text\n",
    "    bsop = BeautifulSoup(text, 'html.parser')\n",
    "    lis = bsop.find('div', {'id': 'wp_page_numbers'}).find('ul').findAll('li')\n",
    "    pages = lis[-1].find('a').attrs['href'].split('.')[0].split('_')[-1]\n",
    "    pages = int(pages)\n",
    "    return pages\n",
    "\n",
    "\n",
    "# 妹子图的首页，用来获取总的页数\n",
    "#first_url = 'http://www.meizitu.com/a/list_1_1.html'\n",
    "first_url = 'http://www.meizitu.com/a/oumei.html'\n",
    "prefix_url = 'http://www.meizitu.com/tag/banluo_5_'\n",
    "\n",
    "# 爬取代理的url地址，选择的是西祠代理\n",
    "url_ip = \"http://www.xicidaili.com/nt/\"\n",
    "\n",
    "# 设定等待时间\n",
    "set_timeout = 10\n",
    "\n",
    "# 爬取代理的页数，2表示爬取2页的ip地址\n",
    "num = 2\n",
    "\n",
    "# 代理的使用次数\n",
    "count_time = 5\n",
    "\n",
    "# 构造headers\n",
    "UserAgent_List = [\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\",\n",
    "    \"Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\"]\n",
    "headers = {\n",
    "    'User-Agent': random.choice(UserAgent_List),\n",
    "    'Accept': \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    'Accept-Encoding': 'gzip',\n",
    "}\n",
    "\n",
    "# 图片存储路径\n",
    "file_path = 'F:\\Meizitu\\Oumei'\n",
    "\n",
    "# 获取总页数\n",
    "pages = get_total_pages(first_url)\n",
    "\n",
    "# 爬取IP代理\n",
    "total_ip = scrawl_ip(url_ip, num)\n",
    "\n",
    "# 带爬取的url\n",
    "url_imgss = download_urls(pages)\n",
    "\n",
    "# url写入地址：\n",
    "file_url = 'oumei.txt'\n",
    "\n",
    "for i in url_imgss:\n",
    "    for j in i:\n",
    "        try:\n",
    "            with open(file_url, 'a') as f:\n",
    "                f.write(j + \"\\n\")\n",
    "                f.close()\n",
    "                print(\"写入%s文件成功\", file_url)\n",
    "        except BaseException:\n",
    "            print(\"写入%s.txt文件失败\", file_url)\n",
    "\n",
    "for url_imgs in url_imgss:\n",
    "    for url_img in url_imgs:\n",
    "        img_list, img_title = scrawl_url(url_img)\n",
    "        if not img_list:\n",
    "            continue\n",
    "        download_img(img_list, img_title)\n",
    "\n",
    "        time.sleep(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
